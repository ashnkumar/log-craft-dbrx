{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1655135-4370-4597-87f4-a3bfca072da6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# [Notebook 1]: Build Vector Store and Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d5c134-51b8-4526-8a55-f86933897447",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install pyth --upgrade --quiet\n",
    "%pip install openai --upgrade --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64ff37bf-7bb3-4855-b32f-fb6d92c7608e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Load in the past maintenance logs along with image filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ce3a43-6f11-40f8-8cd3-f450ddd6916e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'rb') as file:  # The 'rb' argument stands for 'read binary'\n",
    "    full_logs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47752126-b4f3-4137-b048-ad8df7c4ee63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###2. Use DBRX to create short summaries of each log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a13749-e321-4dc3-90e2-28f91a4d640c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "DATABRICKS_ACCESS_TOKEN = \"YOUR_DATABRICKS_TOKEN\"\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=DATABRICKS_ACCESS_TOKEN,\n",
    "  base_url=\"https://dbc-f499a870-66c0.cloud.databricks.com/serving-endpoints\"\n",
    ")\n",
    "\n",
    "def create_summaries_of_prior_logs(openai_client, full_logs):\n",
    "    all_summaries = []\n",
    "    for filename, log in full_logs:\n",
    "        prompt = f\"\"\"\n",
    "            I'll provide you with text for a maintenance inspection report below. Your job is to take this report and provide a summary that is less than 77 tokens and captures the important synthesis of the text. Only consider critical pieces of observations and recommendations. Always remember to include the recommendation (summarized) in your summary. Provide nothing but the summary, don't preface it with anything.\n",
    "\n",
    "            ####\n",
    "            {log}\n",
    "        \"\"\"\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"databricks-dbrx-instruct\",\n",
    "            messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "            ],\n",
    "            max_tokens=4096\n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "        all_summaries.append((filename, summary, log))\n",
    "    return all_summaries\n",
    "\n",
    "all_summaries = create_summaries_of_prior_logs(client, full_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d85076d-1afe-4650-b72d-aded7056a26b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###3. Create multi-modal embeddings using the CLIP model and store them in the vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "809eaf8c-076c-4a34-899b-258c0c410f73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First, load the CLIP model to create image + text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb998350-d92a-4d32-b164-bc42b4782b60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 02:10:55.062737: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51db7908ca1b47acbe18fca1cf81ba0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8fbbaf300f457689fc293b2ee293bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae86ac0d4704d12a045c68e472fe0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a2f6fa8b3b44a2bff9995a9e6db532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba8298e8f8b4f26bc09c08cc2672bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2c80d92b044ebcb62ec6b17c72e480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7676cfd676cb461b87e34229921650f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecb18862a004f389db8c57c7fb65f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "MODEL = 'openai/clip-vit-large-patch14'\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(MODEL)\n",
    "clip_processor = CLIPProcessor.from_pretrained(MODEL)\n",
    "\n",
    "def get_clip_embedding(text, image_path):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = clip_processor(text=[text], images=[image], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = clip_model(**inputs)\n",
    "    image_features = outputs.image_embeds  # This should be the embedding for the image\n",
    "    text_features = outputs.text_embeds    # This should be the embedding for the text\n",
    "\n",
    "    combined_embedding = (image_features + text_features) / 2\n",
    "    return combined_embedding.squeeze().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee380d2f-abb2-421c-8451-990f843676a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Then, create the Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4137cab3-94e7-4ad7-b3e4-c1f6ac36e165",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS past_maintenance_logs (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  summary STRING,\n",
    "  log STRING,\n",
    "  embedding ARRAY<FLOAT>\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b04c13d-addb-458f-ae29-133978136b4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Then cycle through all the embeddings and store them in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f63e1407-bc48-4ae6-8b27-80413122ab9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "spark = SparkSession.builder.appName(\"\").getOrCreate()\n",
    "\n",
    "def embed_and_store_images_summaries(image_path, log, summary):\n",
    "    full_image_path = f\"./images/{image_path}\"\n",
    "    embedding = get_clip_embedding(summary, full_image_path)\n",
    "    data = {\n",
    "        \"summary\": [summary],\n",
    "        \"log\": [log],\n",
    "        \"embedding\": [embedding]\n",
    "    }\n",
    "    pdf = pd.DataFrame(data)\n",
    "    schema = StructType([\n",
    "        StructField(\"summary\", StringType(), True),\n",
    "        StructField(\"log\", StringType(), True),\n",
    "        StructField(\"embedding\", ArrayType(FloatType(), True), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(pdf, schema=schema)\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(\"past_maintenance_logs\")\n",
    "\n",
    "def populate_db(all_summaries):\n",
    "    for filename, summary, log in tqdm(all_summaries, desc=\"Processing summaries\"):\n",
    "        embed_and_store_images_summaries(filename, log, summary)\n",
    "    print(\"Finished storing prior logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8ec65e-27be-49f9-8c85-59d4f23ea5d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing summaries:   0%|          | 0/25 [00:00<?, ?it/s]\rProcessing summaries:   4%|▍         | 1/25 [00:05<02:13,  5.57s/it]\rProcessing summaries:   8%|▊         | 2/25 [00:10<01:55,  5.01s/it]\rProcessing summaries:  12%|█▏        | 3/25 [00:14<01:43,  4.70s/it]\rProcessing summaries:  16%|█▌        | 4/25 [00:19<01:41,  4.82s/it]\rProcessing summaries:  20%|██        | 5/25 [00:23<01:32,  4.62s/it]\rProcessing summaries:  24%|██▍       | 6/25 [00:27<01:24,  4.44s/it]\rProcessing summaries:  28%|██▊       | 7/25 [00:31<01:17,  4.28s/it]\rProcessing summaries:  32%|███▏      | 8/25 [00:35<01:11,  4.22s/it]\rProcessing summaries:  36%|███▌      | 9/25 [00:39<01:06,  4.15s/it]\rProcessing summaries:  40%|████      | 10/25 [00:43<01:01,  4.10s/it]\rProcessing summaries:  44%|████▍     | 11/25 [00:47<00:56,  4.02s/it]\rProcessing summaries:  48%|████▊     | 12/25 [00:51<00:51,  3.94s/it]\rProcessing summaries:  52%|█████▏    | 13/25 [00:55<00:48,  4.00s/it]\rProcessing summaries:  56%|█████▌    | 14/25 [00:59<00:44,  4.08s/it]\rProcessing summaries:  60%|██████    | 15/25 [01:06<00:48,  4.85s/it]\rProcessing summaries:  64%|██████▍   | 16/25 [01:12<00:45,  5.06s/it]\rProcessing summaries:  68%|██████▊   | 17/25 [01:17<00:40,  5.10s/it]\rProcessing summaries:  72%|███████▏  | 18/25 [01:22<00:36,  5.21s/it]\rProcessing summaries:  76%|███████▌  | 19/25 [01:27<00:31,  5.22s/it]\rProcessing summaries:  80%|████████  | 20/25 [01:33<00:26,  5.31s/it]\rProcessing summaries:  84%|████████▍ | 21/25 [01:37<00:19,  5.00s/it]\rProcessing summaries:  88%|████████▊ | 22/25 [01:41<00:14,  4.76s/it]\rProcessing summaries:  92%|█████████▏| 23/25 [01:46<00:09,  4.56s/it]\rProcessing summaries:  96%|█████████▌| 24/25 [01:50<00:04,  4.47s/it]\rProcessing summaries: 100%|██████████| 25/25 [01:55<00:00,  4.56s/it]\rProcessing summaries: 100%|██████████| 25/25 [01:55<00:00,  4.61s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished storing prior logs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "populate_db(all_summaries)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3278995887959789,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Build-vectore-store-and-index",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
