{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e85dde5b-d11c-4645-96c0-27f01705ff5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# [Notebook 2]: Inference Phase (Multi-modal RAG)\n",
    "For the indexing phase, see notebook \"Build-vector-store-and-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb723578-ede4-4721-86a1-b8c4ba3a1126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install openai --upgrade --quiet\n",
    "%pip install mlflow[genai]>=2.9.0 --quiet\n",
    "%pip install FPDF --quiet\n",
    "%pip install --upgrade pydantic --quiet\n",
    "%pip install --upgrade gradio --quiet\n",
    "%pip install databricks.vectorsearch --quiet\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efbfb167-7195-4c55-86c7-c7812f7cfdff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "DATABRICKS_ACCESS_TOKEN = f\"YOUR_DATABRICKS_TOKEN\"\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=DATABRICKS_ACCESS_TOKEN,\n",
    "  base_url=\"https://dbc-f499a870-66c0.cloud.databricks.com/serving-endpoints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "717151e3-2025-4144-b472-f70b3203c25a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First load the same CLIP model that we used in the indexing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d309b92c-a697-4f72-9237-ad8831c93fe9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 16:36:15.526283: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "MODEL = 'openai/clip-vit-large-patch14'\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(MODEL)\n",
    "clip_processor = CLIPProcessor.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca569daf-54e5-4e58-8f72-3b31c73753a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Then access the vector search endpoint and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1979cd96-e448-4574-b522-df0e585a743a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n"
     ]
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = 'YOUR_ENDPOINT'\n",
    "VS_INDEX_FULLNAME = 'YOUR_INDEX'\n",
    "vsc = VectorSearchClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec58dc20-1e13-4f01-b124-493c17c1daf0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Build functions to get the CLIP embedding, retrieve relevant logs, build the final prompt and send to GPT4-Vision. Also function for creating the final PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56dbb12-c4ec-4601-aa17-03d163ccd480",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "from fpdf import FPDF\n",
    "import tempfile\n",
    "import datetime\n",
    "\n",
    "def retrieve_and_prepare_prompt(user_text, logs):\n",
    "    combined_full_logs = \"\\n\\n\".join([f\"LOG {i+1}:\\n{logs}\" for i, logs in enumerate(logs)])\n",
    "    \n",
    "    # Prepare the prompt for GPT-4-Vision\n",
    "    prompt = f'''\n",
    "    There is a field service engineer and have been sent out to inspect a manufacturing-based machine out int he field and provide a maintenance log. He jotted down a few notes but wants you to write out the full log.\n",
    "\n",
    "    He has given you an image of the machine and his jotted-down notes.\n",
    "\n",
    "    Luckily, you have access to prior maintenance logs and associated images. Based on the image and his notes, you have retrieved the following full logs that are most similar to the combination of image and notes he has given you.\n",
    "\n",
    "    Based on what you see in the logs, plus the attached image, plus the jotted down notes below, write a full maintenance log in the same format as the logs.\n",
    "\n",
    "    Each log has to have the 3 sections: 1) Inspection Summary, 2) Technical Evaluation, 3) Recommendations.\n",
    "\n",
    "    =====\n",
    "    \n",
    "    Image: (attached)\n",
    "    User notes: {user_text}\\n\n",
    "    \n",
    "    Combined full logs of similar prior entries: \\n\n",
    "    {combined_full_logs}\n",
    "    \n",
    "    ====\n",
    "    Generate a full log based on the above information that resembles the format of the combined full logs given and leverages the text in those logs plus the attached image to help you generate this new log. Use the retrieved logs mostly for informing how you format your generated log and if there are relevant pieces of information in those logs that can help you, feel free to leverage them but make sure you place a lot of emphasis on the attached image.\n",
    "    '''\n",
    "    return prompt\n",
    "\n",
    "def get_clip_embedding(text, image_input):\n",
    "    # Check if the input is a NumPy array\n",
    "    if isinstance(image_input, np.ndarray):\n",
    "        # Convert NumPy array to PIL Image\n",
    "        image = Image.fromarray(np.uint8(image_input)).convert('RGB')\n",
    "    else:\n",
    "        # Load image from the path\n",
    "        image = Image.open(image_input).convert('RGB')\n",
    "    \n",
    "    inputs = clip_processor(text=[text], images=[image], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = clip_model(**inputs)\n",
    "    image_features = outputs.image_embeds  # This should be the embedding for the image\n",
    "    text_features = outputs.text_embeds    # This should be the embedding for the text\n",
    "    combined_embedding = (image_features + text_features) / 2\n",
    "    return combined_embedding.squeeze().detach().numpy()\n",
    "\n",
    "def get_relevant_docs(embedding):\n",
    "    query_vector = embedding.tolist() if isinstance(embedding, np.ndarray) else embedding\n",
    "    results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, VS_INDEX_FULLNAME).similarity_search(\n",
    "        query_vector=query_vector,\n",
    "        columns=[\"log\"],\n",
    "        num_results=3)\n",
    "    docs = results.get('result', {}).get('data_array', [])\n",
    "    return docs\n",
    "\n",
    "def encode_the_image(image_input):\n",
    "    # Check if the input is a NumPy array\n",
    "    if isinstance(image_input, np.ndarray):\n",
    "        # Convert NumPy array to PIL Image\n",
    "        image = Image.fromarray(np.uint8(image_input)).convert('RGB')\n",
    "    else:\n",
    "        # Load image from the path\n",
    "        image = Image.open(image_input).convert('RGB')\n",
    "    \n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format=\"PNG\")  # Saving the image in PNG format\n",
    "    buffer.seek(0)\n",
    "    img_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
    "    return img_base64\n",
    "\n",
    "def get_gpt4_generate_log(client, prompt, base64_encoded_image):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_encoded_image}\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "    ],\n",
    "    model=\"ak-openai-gpt4v\",\n",
    "    max_tokens=4096\n",
    "    )\n",
    "    # print(chat_completion.choices[0].message.content)\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def generate_log_for_user(client, image, notes):\n",
    "    try:\n",
    "        print(\"Getting embedding ...\")\n",
    "        embedding = get_clip_embedding(notes, image)  # Ensure this function is properly defined and imported\n",
    "        print(\"Getting similar logs ...\")\n",
    "        similar_logs = get_relevant_docs(embedding)  # Ensure this function is properly defined and imported\n",
    "        print(\"Encoding image ...\")\n",
    "        base64_encoded_image = encode_the_image(image)  # Ensure this function is properly defined and imported\n",
    "        print(\"Building full prompt ...\")\n",
    "        full_prompt = retrieve_and_prepare_prompt(notes, similar_logs)  # Ensure this function is properly defined and imported\n",
    "        print(\"Generating log ...\")\n",
    "        generated_log = get_gpt4_generate_log(client, full_prompt, base64_encoded_image)  # Ensure this function is properly defined and imported\n",
    "        return generated_log\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_log_for_user: {e}\")\n",
    "        return str(e)  # Return error as string for debugging purposes\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font('Arial', 'B', 16)\n",
    "        self.cell(0, 10, 'Maintenance log 5/5/24', 0, 1, 'C')\n",
    "\n",
    "    def chapter_title(self, title):\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, title, 0, 1, 'L')\n",
    "        self.ln(2)  # Small line break after the title\n",
    "\n",
    "    def chapter_body(self, body):\n",
    "        self.set_font('Arial', '', 12)\n",
    "        self.multi_cell(0, 5, body, align='L')\n",
    "        self.ln()\n",
    "\n",
    "    def add_image(self, image_input):\n",
    "        if isinstance(image_input, np.ndarray):\n",
    "            image = Image.fromarray(image_input.astype('uint8')).convert('RGB')\n",
    "        else:\n",
    "            image = Image.open(image_input).convert('RGB')\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=False) as tmp_image:\n",
    "            image.save(tmp_image, format=\"JPEG\")\n",
    "            tmp_image.flush()  # Ensure all data is written to file\n",
    "            x_center = (210 - 100) / 2  # Center the image\n",
    "            self.image(tmp_image.name, x=x_center, w=100)\n",
    "            self.ln(10)  # Space after the image\n",
    "\n",
    "def create_pdf(image_input, generated_log, notes):\n",
    "    pdf = PDF()\n",
    "    pdf.add_page()\n",
    "    pdf.add_image(image_input)\n",
    "\n",
    "    # Custom parsing and formatting of the log text\n",
    "    sections = [\"Inspection Summary\", \"Technical Evaluation\", \"Recommendations\"]\n",
    "    start = 0\n",
    "    for section in sections:\n",
    "        start_idx = generated_log.find(section, start)\n",
    "        if start_idx != -1:\n",
    "            end_idx = generated_log.find(\"\\n\", start_idx)\n",
    "            if end_idx == -1:\n",
    "                end_idx = len(generated_log)\n",
    "            pdf.chapter_title(generated_log[start_idx:end_idx])\n",
    "            body_start = end_idx + 1\n",
    "            body_end = min([generated_log.find(s, body_start) for s in sections if generated_log.find(s, body_start) != -1] + [len(generated_log)])\n",
    "            pdf.chapter_body(generated_log[body_start:body_end].strip())\n",
    "            start = body_end\n",
    "\n",
    "    # Dynamic filename generation with timestamp\n",
    "    current_time = datetime.datetime.now().strftime(\"%H-%M-%S\")\n",
    "    pdf_output = f\"./pdf_{current_time}.pdf\"\n",
    "    pdf.output(pdf_output)\n",
    "    return pdf_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e9ada0f-9d1d-43ca-b3f8-1e15a4894dcb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creates Gradio app for use within Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24afa978-3123-490f-ac3f-705c36a130a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    iframe { width: 100% !important; height: 900px !important; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\nRunning on public URL: https://b9c8ee6717ceb38b7a.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b9c8ee6717ceb38b7a.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Custom CSS to adjust the iframe width and height within the Jupyter notebook\n",
    "custom_css = \"\"\"\n",
    "<style>\n",
    "    iframe { width: 100% !important; height: 900px !important; }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# Display the custom CSS\n",
    "display(HTML(custom_css))\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as app:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type=\"numpy\", label=\"Upload Image\")\n",
    "            text_input = gr.Textbox(label=\"Raw maintenance notes\", placeholder=\"Enter maintenance notes here...\")\n",
    "        with gr.Column():\n",
    "            log_output = gr.Textbox(label=\"Generated log shows up here\", interactive=False, value=\"\", placeholder=\"...\")\n",
    "    \n",
    "    generate_log_button = gr.Button(\"Generate Log\")\n",
    "    download_pdf_button = gr.Button(\"Download PDF\")\n",
    "\n",
    "    def on_download_pdf(image, log, notes):\n",
    "        time.sleep(2)\n",
    "        pdf_content = create_pdf(image, log, notes)\n",
    "        return pdf_content\n",
    "    \n",
    "    generate_log_button.click(\n",
    "        generate_log_for_user,\n",
    "        inputs=[client, image_input, text_input],\n",
    "        outputs=log_output\n",
    "    )\n",
    "    \n",
    "    download_pdf_button.click(\n",
    "        on_download_pdf,\n",
    "        inputs=[image_input, log_output, text_input],\n",
    "        outputs=None\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        generate_log_button\n",
    "        download_pdf_button\n",
    "\n",
    "app.launch(\n",
    "    share=True,\n",
    "    inline=True,            # Ensure the interface launches within the notebook cell\n",
    "    width='100%',           # Set the width to 100% of the cell\n",
    "    height=800             # Adjust the height to avoid scrolling (set as per your content)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Deployed-RAG-product",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
